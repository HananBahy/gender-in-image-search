<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Data and analysis for <em>Unequal representation and gender stereotypes in image search results for occupations</em></title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>



<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>Data and analysis for <em>Unequal representation and gender stereotypes in image search results for occupations</em></h1>

<p><a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img
alt="This work is licensed under a Creative Commons Attribution 4.0 International License" 
title="This work is licensed under a Creative Commons Attribution 4.0 International License" 
src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a> <em>Matthew Kay (<a href="mailto:mjskay@uw.edu">mjskay@uw.edu</a>), Cynthia Matuszek (<a href="mailto:cmat@umbc.edu">cmat@umbc.edu</a>), and Sean Munson (<a href="mailto:smunson@uw.edu">smunson@uw.edu</a>)</em></p>

<p>This repository contains data and analysis code from: </p>

<p>Kay, Matthew, Matuszek, Cynthia, and Munson, Sean. Unequal Representation and
Gender Stereotypes in Image Search Results for Occupations. <em>CHI 2015:
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>
(upcoming). <a href="http://dx.doi.org/10.1145/2702123.2702520">http://dx.doi.org/10.1145/2702123.2702520</a></p>

<p>This file is structured as an extended version of the above paper, with inline
R code for running the analyses. It is generated by README.Rmd (an <a href="http://rmarkdown.rstudio.com/">RMarkdown</a> file). The intent is to allow both for
reproduction of our analyses, as well as replication and extension of our
analyses. We would love to hear about any new analyses you conduct based on this
work, or problems/improvements you have with the analyses contained herein.
Please email us!</p>

<h2>Citing this work</h2>

<p>Please cite the CHI paper above.</p>

<h2>Libraries needed for the analyses in this paper</h2>

<pre><code class="r">library(Matching)   #ks.boot
library(visreg)
library(boot)       #logit, inv.logit (could use qlogis/plogis but this is clearer) 
library(pscl)       #vuong
library(plyr)       #**ply
library(ordinal)    #clmm
</code></pre>

<h1>Unequal representation and gender stereotypes in image search results for occupations</h1>

<h2>Abstract</h2>

<p>Information environments have the power to affect people’s perceptions and
behaviors. In this paper, we present the results of studies in which we
characterize the gender bias present in image search results for a variety of
occupations. We experimentally evaluate the effects of bias in image search
results on the images people choose to represent those careers and on people’s
perceptions of the prevalence of men and women in each occupation. We find
evidence for both stereotype exaggeration and systematic underrepresentation of
women in search results. We also find that people rate search results higher
when they are consistent with stereotypes for a career, and shifting the
representation of gender in image search results can shift people’s perceptions
about real-world distributions. We also discuss tensions between desires for
high-quality results and broader societal goals for equality of representation
in this space.</p>

<h2>Introduction</h2>

<p>Every day, billions of people interact with interfaces that help them access
information and make decisions. As increasing amounts of information become
available, systems designers turn to algorithms to select which information to
show to whom. These algorithms and the interfaces built on them can influence
people’s behaviors and perceptions about the world. Both algorithms and
interfaces, however, can be biased in how they represent the world [10,35].
These biases can be particularly insidious when they are not transparent to the
user or even to the designer [29]. The information people access affects their
understanding of the world around them and the decisions they make: biased
information can affect both how people treat others and how they evaluate their
own choices or opportunities.</p>

<p>One of the most prevalent and persistent biases in the United States is a bias
against women with respect to occupational choices, opportunities, and
compensation [21,27]. Stereotypes of many careers as gender-segregated serve to
reinforce gender sorting into different careers and unequal compensation for men
and women in the same career. Cultivation theory, traditionally studied in the
context of television, contends that both the prevalence and characteristics of
media portrayals can develop, reinforce, or challenge viewers’ stereotypes [30].</p>

<p>Inequality in the representation of women and minorities, and the role of
online information sources in portraying and perpetuating it, have not gone
unnoticed in the technology community. This past spring, Getty Images and
LeanIn.org announced an initiative to increase the diversity of working women
portrayed in the stock images and to improve how they are depicted [28]. A
recent study identified discrimination in online advertising delivery: when
searching for names, search results for black-identifying first names were
accompanied by more ads for public records searches than those for
white-identifying first names, and those results were more likely to suggest
searches for arrest records [35]. These findings raise questions about the
possible impacts of this discrimination and how to design technology in
consideration of issues such as structural racism.</p>

<p>Despite efforts to address some of these issues, there has been limited public
effort to measure how online information sources represent men and women.
Further, we do not know how people perceive these biases when they view
information sources, or the extent to which it affects their choices or
perceptions about the world. For example, are gender distributions in search
results representative of those in the real world – and if not, how does that
affect people’s perceptions of the world?</p>

<p>In this paper, we begin to address these gaps through four studies
characterizing how genders are represented in image search results for
occupations. We evaluate whether and how these biases affect people’s
perceptions of search result quality, their beliefs about the occupations
represented, and the choices they make. In a series of studies on existing image
search results, manipulated search results, and people’s perceptions of these
results, we investigate the following phenomena:</p>

<ul>
<li><p><em>Stereotype exaggeration</em>: While gender proportions in image search results
are close to those in actual occupations, results for many occupations exhibit
a slight exaggeration of gender ratios according to stereotype: e.g.,
male-dominated professions tend to have even more men in their results than
would be expected if the proportions reflected real-world distributions. This
effect is also seen when people rate the quality of search results or select
the best image from a result: they prefer images with genders that match the
stereotype of an occupation, even when controlling for qualitative differences
in images.</p></li>
<li><p><em>Systematic over-/under- representation</em>: Search results also exhibit a
slight under-representation of women in images, such that an occupation with
50% women would be expected to have about 45% women in the results on average.
However, when evaluating image result quality, people do not systematically
prefer either gender: instead, stereotyping dominates, and they prefer images
that match a given occupation’s gender stereotype.</p></li>
<li><p><em>Qualitative differential representation</em>: Image search results also exhibit
biases in how genders are depicted: those matching the gender stereotype of a
profession tend to be portrayed as more professional-looking and less
inappropriate-looking.</p></li>
<li><p><em>Perceptions of occupations in search results</em>: We find that people’s
existing perceptions of gender proportions in occupations are quite accurate
(R<sup>2</sup> of 0.72), but that manipulated search results can have a small
but significant effect on perceptions, shifting estimations on average ~7%.</p></li>
</ul>

<p>This last point contributes to the broader motivation of this work: not only to
contribute to an understanding of how everyday information systems – here, image
search results – both reflect and influence perceptions about gender in
occupations, but also to characterize a possible design space for correcting or
adjusting for differences in representation. We do not take a stance on whether
or how designers and system builders should address gender inequality and its
effects in their systems, but we believe that designers should be aware of
inequalities in their systems and how those inequalities can affect perceptions.
We particularly note two overriding design tensions in this space: the desire to
improve perceived search result quality, and societal motivations for improving
equality of representation.</p>

<p>In the remainder of this paper, we review motivating work and our specific
research questions. We then describe four studies and their answers to these
research questions before discussing the implications for designers and society.</p>

<h2>Background and motivation</h2>

<p>The Internet and large data sets create many new opportunities for engaging
with data and using it in communication and to support decision making. They
also come with challenges and pitfalls. A recent White House report noted that
biases in data collection and presentation can lead to flawed understandings of
the need for and use of public services, and that this can lead to
discrimination in who receives those services [9].</p>

<p>In the studies presented in this paper, we investigate the prevalence and risks
of gender-based stereotyping and bias in image search results for occupations.
Our research questions were guided by prior work in stereotyping and biases, the
role of media in forming, perpetuating, or challenging these, and contemporary
discussions of the effects of stereotypes and biases information environments.</p>

<h3>Stereotypes and bias</h3>

<p>A stereotype refers to a belief that individuals in a group – e.g., gender,
occupation, race, ethnicity, or particular background – generally have one or
more traits or behaviors. People make use of stereotypes to explain their own or
others’ behaviors [15,36], to justify actions or decide how to act [4,36], and
to define group boundaries [36]. While accurate stereotypes may be useful for
making decisions in the absence of more specific information, inaccurate
stereotypes can be harmful. Belief that one’s group performs poorly at a task
can lead to lower performance (stereotype threat [33]). Stereotyped expectations
about someone’s behavior can also lead them to behave in that way, a
self-fulfilling prophecy [33,39], and expectations about one’s own abilities can
influence aspirations and choices, such as beliefs about what career path one
should follow [7,8].</p>

<p>Bias arises when an individual, group or process unfairly and systematically
treats an individual or group favorably or unfavorably. Stereotypes about
abilities or character are a common source of bias [18], often to the
disadvantage of a particular race, sexual orientation, or gender. For example,
stereotypes about gender and parental roles can systematically limit women’s
career advancement [14,16,17].</p>

<h3>Effects of stereotypes and bias in the media</h3>

<p>The portrayal of women and racial/ethnic minorities in television and other
media has received considerable attention as both a possible source of
stereotypes and opportunity to challenge them [12]. Exclusion of these groups
can imply that they are “unimportant, inconsequential, and powerless” [12].
Their inclusion offers specific examples whose implications depend on how they
are portrayed, and these portrayals can reinforce or challenge stereotypes.
Unfortunately, portrayals often reinforce negative stereotypes, for example by
showing racial/ethnic minorities as criminals, victims of criminals, and in
low-status service jobs [12].</p>

<p>Cultivation theory predicts that television’s portrayal of the world affects
people’s beliefs about reality [11,32]. Portrayals, or the lack of portrayals,
can affect whether people believe that people like them commonly participate in
an occupation, or their perceived self-efficacy for that role [12,32].
Researchers studying television commercials find that women are less likely to
be portrayed as workers and that they exaggerate gender-occupation stereotypes [6].
They express concern that such portrayals may perpetuate stereotypes.
Cultivation theory has also been found to predict how people perceive risks
after experiencing them in a video game [38], and playing a sexualized female
character reduces female players’ feelings of self-efficacy [3].</p>

<h3>Stereotypes and bias in information systems</h3>

<p>Like media and other built systems or environments, computer systems have bias.
Friedman and Nissenbaum describe biased computer systems as those that
“systematically and unfairly discriminate against certain individuals or groups
of individuals in favor of others” [10]. They describe three categories:
preexisting bias (arising from biases present in individuals or society),
technical bias (arising from technical constraints), and emergent bias (arising
in real use, in which a system is mismatched for the capabilities or values of
its users). They argue: “freedom from bias should be counted among the select
set of criteria according to which the quality of systems in use in society
should be judged.”</p>

<p>Search engines have been studied and received popular attention for bias in
their results, both for what they index and present overall [20,37] and what they
present to particular users [29]. People tend to rely on search engines’
selection and ordering of results as signs of quality and relevance [22,23], and
so biased search results may affect people’s choices and beliefs. Scholars have
previously noted bias in which geographic locations are indexed and listed [37].
Others express concern that search autocomplete features could perpetuate
preexisting biases, noting that suggestions varied between different religious
groups, and sexual and racial minorities received more negatively framed
questions as suggestions [2]. As illustrated by these examples, a search engine
which has neither algorithms that systematically favor one group nor designers
with a particular bias can still perpetuate preexisting societal biases: a
representative indexing of biased source material will produce results that
contain the same biases.</p>

<p>More recently, Getty Images and Sheryl Sandberg’s Lean In Foundation announced
an effort to improve the depiction of working women in stock photos. They argue
that existing images support stereotypes of working women as sidelined,
sexualized, or in supporting roles, and that these depictions hurt women’s
career aspirations and prospects [13,25,28].</p>

<h2>Research questions</h2>

<p>Motivated by these concerns and questions about them, we conducted a series of
studies to evaluate bias in image search results. Pre-existing biases that
affect the images available for image search systems, and algorithms designed to
represent available content, may lead to biased result sets, which in turn
affect people’s perceptions and choices among the search results. We
specifically focus on gender representation in image search results for
occupations. We choose the portrayal of occupations because it is a topic of
societal importance that has recently received attention and efforts to
ameliorate biases. While efforts such as the partnership between Getty Images
and Lean In may make more diverse or positive images available, and particularly
to those who access the Lean In collection, many people turn to major search
engines when looking to illustrate a topic, and so we focus our attention on the
image search results for a major search engine.</p>

<p>To the discussion of the bias in computer systems, we contribute an assessment
of the current extent and form of several forms of stereotyping and differences
of representation present in image search results: <em>stereotype exaggeration</em>,
<em>systematic over-/under-representation</em>, and <em>qualitative differential
representation</em>. We also explore the effects of these biases on perceptions of
the occupations in question. We designed four studies to answer these research
questions:</p>

<ul>
<li><p>Study 1: How does the prevalence of men and women in image search results for
professions correspond to their prevalence in actual professions? Are genders
systematically over or underrepresented across careers, and is there
stereotype exaggeration in gender proportions?</p></li>
<li><p>Study 2: Are there qualitative differences in how men and women are portrayed
in the image search results?</p></li>
<li><p>Study 3: Do biased image search results lead people to perpetuate a bias in
image search results when they choose images to represent a profession (i.e.
through stereotype exaggeration)? Are there systemic over- or
under-representations of women in preferred results? How do differences in
representation affect people’s perceptions of the search result quality?</p></li>
<li><p>Study 4: Do differences in representation in image search results affect
viewers’ perceptions of the prevalence of men and women in that occupation?
Can we shift those opinions by manipulating results?</p></li>
</ul>

<p>For all studies, we recruited turkers/participants<sup><a
name="footnote1_source" href="#footnote1">1</a></sup> from Amazon’s Mechanical
Turk microtask market. We required that they be from the United States (as our
occupation prevalence data is specific to that population) and, for studies 2-4,
required them to have the Masters qualification.</p>

<h2>Study 1: Gender proportions in results compared to actual proportions</h2>

<p>In this study, we sought to characterize the extent to which the prevalence of
men and women in image search results for professions correspond to their actual
prevalence in those occupations. As a gold standard for actual prevalence of men
and women by occupation, we used estimates from the US Bureau of Labor and
Statistics (BLS) [5]. We did not use all occupations, but removed occupations
that:</p>

<ul>
<li><p>Presented difficult polysemy problems: for example, occupations that are
listed as conjunctions of multiple occupations in the BLS, such as “Musicians,
singers, and related workers”, are difficult to reduce to a single search.</p></li>
<li><p>Had non-obvious search terms: for example, “Miscellaneous media and
communication workers”.</p></li>
<li><p>Are typically referred to using gender-specific terms: for example, “tailor”
and “seamstress”</p></li>
</ul>

<p>Most of the remaining terms had straightforward translations from BLS
categories into search terms for a worker in that occupation (for example, we
mapped “Nurse practitioners” in the BLS database to the search term “nurse
practitioner”). Some categories required limited interpretation (e.g., we
translated “Police and sheriff’s patrol officers” into “police officer”); for
these terms, all three authors had to agree on a corresponding search term for
the category to be included. This left us with 96 occupations having an entry in
BLS and a corresponding search term. The full BLS dataset is available in
<code>data/public/bls_occupations.csv</code> and the set of 96 occupations chosen for 
labelling is in <code>data/public/labelled_bls_occupations.csv</code>.</p>

<p>We then downloaded the top 100 Google Image search results for each search term
(from July 26–29, 2013). For each image, turkers were asked to indicate whether
there were no people, one person, or more than one person in the image. They
were also asked whether the people were women, men, children, or of unknown
gender (and to check all that apply).<sup><a name="footnote2_source"
href="#footnote2">2</a></sup> We had three turkers label each image. The 
mturk template for that task is available at <code>mturk/label_gender_task.html</code>.</p>

<h3>Results</h3>

<h4>Representativeness of filtered dataset</h4>

<p>A requirement of this study was to obtain a representative dataset of images of
individuals in different occupations with properly labelled gender. This
required some filtering to ensure that images had correctly labelled genders,
depicted only people of that gender, and were generally images of people in the
first place. To label gender, we took the majority label for each image, and
dropped those images from the results which did not have majority agreement. We
then dropped entire search terms which:</p>

<ul>
<li><p>Had less than 80% of the images labelled with majority agreement (two terms
failed this criterion: firefighter and baker; notably, firefighter had only
64% agreement, largely because most of the images were dark silhouettes of
uniformed firefighters with ambiguous gender, frequently labeled as “male” by
some turkers).</p></li>
<li><p>Had few images containing only one gender or that mostly depicted workers
with clients/patients. For example, hairdresser was dropped since too many of
its images contained both hairdresser and client, making it difficult to
determine which gender label corresponds to which. We considered asking
turkers whether the person in question has the given occupation; however, this
implicitly asks them to decide if a person of that gender could be a
hairdresser (thus potentially subject to gender stereotypes related to that
profession, which would bias our labelled data set), so we opted to filter
occupations with multiple genders in the majority of images.</p></li>
<li><p>Had too few people in the image results; e.g., dishwasher largely returned
images of dishwashing machines.</p></li>
<li><p>Corresponded with a common name (e.g., baker returned many results of people
with the surname Baker).</p></li>
</ul>

<p>This second filtering process left us with 45 occupations. This dataset is
is <code>data/public/filtered_bls_occupations.csv</code>. To ensure that all
levels of our filtering (from the initial selection of search terms down to the
filtering of labelled images) had not biased the representativeness of our final
selection of occupations in terms of gender or ethnicity, we conducted a series
of Kolmogorov-Smirnov tests comparing the gender and ethnicity distributions of
the filtered 45 occupations to the entire set of 535 occupations in the BLS
(using bootstrap p values; unlike the traditional KS test this allows for
non-continuous distributions and ties):</p>

<pre><code class="r">occupations = read.csv(&quot;data/public/bls_occupations.csv&quot;)
filtered_occupations = read.csv(&quot;data/public/filtered_bls_occupations.csv&quot;)

for (colname in c(&quot;p_women&quot;, &quot;p_asian&quot;, &quot;p_black&quot;, &quot;p_hispanic&quot;)) {
    #plot CDF comparison for this group
    plot(ecdf(occupations[[colname]]), col=&quot;red&quot;, pch=NA, 
        main=paste(&quot;Comparison of ECDFs\nof&quot;, colname))
    lines(ecdf(filtered_occupations[[colname]]), verticals=TRUE, 
        pch=NA, col=&quot;black&quot;)
    legend(&quot;bottomright&quot;, c(&quot;Full&quot;, &quot;Filtered&quot;), col=c(&quot;red&quot;, &quot;black&quot;), 
        lty=&quot;solid&quot;, lwd=1, text.col=&quot;black&quot;, bty=&quot;n&quot;)

    #run KS test (default formatting leaves something to be desired,
    #so we&#39;ll give more detail ourselves here)
    ks.result = ks.boot(filtered_occupations[[colname]], occupations[[colname]])    
    cat(&quot;Two-sample Kolmogorov-Smirnov test\n&quot;,
        &quot;Null hyp: distribution of &quot;, colname, &quot; for occupations in full dataset\n&quot;,
        &quot;          is the same as the distribution of &quot;, colname, 
        &quot; for occupations in filtered dataset\n&quot;,
        &quot;D_&quot;, nrow(filtered_occupations), &quot;,&quot;, nrow(occupations), &quot; = &quot;, 
        ks.result$ks$statistic[1], &quot;\n&quot;, sep=&quot;&quot;)
    print(summary(ks.result))
}
</code></pre>

<p><img src="data:image/png;base64,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" alt="plot of chunk ks_tests"/> </p>

<pre><code>## Two-sample Kolmogorov-Smirnov test
## Null hyp: distribution of p_women for occupations in full dataset
##           is the same as the distribution of p_women for occupations in filtered dataset
## D_45,535 = 0.0996997
## 
## Bootstrap p-value:     0.769 
## Naive p-value:         0.82557 
## Full Sample Statistic: 0.0997
</code></pre>

<p><img src="data:image/png;base64,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" alt="plot of chunk ks_tests"/> </p>

<pre><code>## Two-sample Kolmogorov-Smirnov test
## Null hyp: distribution of p_asian for occupations in full dataset
##           is the same as the distribution of p_asian for occupations in filtered dataset
## D_45,535 = 0.09009009
## 
## Bootstrap p-value:     0.837 
## Naive p-value:         0.90448 
## Full Sample Statistic: 0.09009
</code></pre>

<p><img src="data:image/png;base64,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" alt="plot of chunk ks_tests"/> </p>

<pre><code>## Two-sample Kolmogorov-Smirnov test
## Null hyp: distribution of p_black for occupations in full dataset
##           is the same as the distribution of p_black for occupations in filtered dataset
## D_45,535 = 0.1021021
## 
## Bootstrap p-value:     0.731 
## Naive p-value:         0.80298 
## Full Sample Statistic: 0.1021
</code></pre>

<p><img src="data:image/png;base64,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" alt="plot of chunk ks_tests"/> </p>

<pre><code>## Two-sample Kolmogorov-Smirnov test
## Null hyp: distribution of p_hispanic for occupations in full dataset
##           is the same as the distribution of p_hispanic for occupations in filtered dataset
## D_45,535 = 0.1423423
## 
## Bootstrap p-value:     0.34 
## Naive p-value:         0.39797 
## Full Sample Statistic: 0.14234
</code></pre>

<p>We did not find evidence that our filtered dataset significantly differed from
the set of occupations in the BLS in terms of gender distribution, distribution
of Asian people, distribution of Black or African American people, or
distribution of Hispanic or Latino people. Note the close correspondence of
empirical cumulative distribution functions for the filtered dataset versus the
full BLS dataset.</p>

<h4>Misrepresentation of genders in search results</h4>

<p>We ran several models to assess the possibility of systematic differences of
representation in depictions of occupations in image search results compared to
the known proportions in the BLS. The purpose of these models was to assess the
presence of two potential forms of quantitative differences in representation.</p>

<p>The first is that gender proportions are exaggerated according to stereotypes
for each career (stereotype exaggeration). For example, if stereotyping occurs
in image results, we would expect a profession with 75% males in the BLS to have
more than 75% males in the image results. The second possibility is that there
is a systematic overrepresentation of one gender, across all careers, in the
search results.</p>

<h4>Stereotype exaggeration by career</h4>

<p>To assess whether men or women were over- or under-represented according to
stereotypes for careers, we ran two logistic regression models: a stereotyped
and a non-stereotyped model. The stereotyped model regressed the logit of the
proportion of women in the search results on the proportion of women in BLS:</p>

<pre><code class="r">proportions = read.csv(&quot;data/public/filtered_bls_occupations_with_search_gender_proportions.csv&quot;)

#we set the x-intercept to -0.5 so that the model intercept
#represents deviation from equal representation (note that
#logit(0.5) = 0, so if there is no systematic over/under
#representation we would expect the intercept of this model to
#be ~0). This is mainly a convenience for the analysis below
#as even if we did not do this an equivalent model would result
#(just with slightly harder to interpret coefficients).
m.stereotyped = glm(search_p_women ~ I(bls_p_women - 0.5), 
    family=quasibinomial, data=proportions, weights=search_n)
</code></pre>

<p>This model exhibits an s-curve characteristic of the logit of a proportion and
indicative of stereotyping: extreme gender proportions in the BLS are pulled
even more to the extremes in the search results:</p>

<pre><code class="r">#plot fit and points
visreg(m.stereotyped, scale=&quot;response&quot;, rug=FALSE)
points(search_p_women ~ bls_p_women, data=proportions, pch=20)
#add some reference lines
abline(coef=c(0,1), lty=&quot;dashed&quot;)
abline(v=.5, lty=&quot;dashed&quot;)
</code></pre>

<p><img src="data:image/png;base64,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" alt="plot of chunk stereotyped_model_plot"/> </p>

<p>The non-stereotyped model regressed the logit of the proportion of women in the
search results on the logit of the proportion of women in BLS, thus not
exhibiting an s-curve:</p>

<pre><code class="r">m.nonstereotyped = glm(search_p_women ~ logit(bls_p_women),
    family=quasibinomial, data=proportions, weights=search_n)

#plot fit and points
visreg(m.nonstereotyped, scale=&quot;response&quot;, rug=FALSE)
points(search_p_women ~ bls_p_women, data=proportions, pch=20)
#add some reference lines
abline(coef=c(0,1), lty=&quot;dashed&quot;)
abline(v=.5, lty=&quot;dashed&quot;)
</code></pre>

<p><img src="data:image/png;base64,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" alt="plot of chunk nonstereotyped_model"/> </p>

<p>While both models can account for a systematic over-representation of one
gender across careers, only the stereotyped model can account for the pulling at
the extremes characteristic of stereotyping by the typical gender of a career.</p>

<p>We found some evidence for stereotype exaggeration: the stereotyped model had
qualitatively better residual fit, which we can see by comparing plots of the
residual fits:</p>

<pre><code class="r">plot(m.stereotyped, which=1)
</code></pre>

<p><img src="data:image/png;base64,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" alt="plot of chunk nonstereotyped_model_plot"/> </p>

<pre><code class="r">plot(m.nonstereotyped, which=1)
</code></pre>

<p><img src="data:image/png;base64,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" alt="plot of chunk nonstereotyped_model_plot"/> </p>

<p>We can also compare the models using Vuong’s closeness test for model
fit:</p>

<pre><code class="r">#N.B. we refit binomial models instead of quasibinomial because
#Vuong&#39;s test requires MLE-based models. We also use the
#direct (case-by-case) data rather than a data table of 
#proportions and counts because the vuong function requires it.
images = read.csv(&quot;data/public/gender_labelled_images.csv&quot;)
m.stereotyped.2 = glm(image_gender ~ I(bls_p_women - 0.5), family=binomial, data=images)
m.nonstereotyped.2 = glm(image_gender ~ logit(bls_p_women), family=binomial, data=images)

#stop if the coefficients of these models aren&#39;t the same as the models fit above
stopifnot(all.equal(coef(m.stereotyped), coef(m.stereotyped.2))) 
stopifnot(all.equal(coef(m.nonstereotyped), coef(m.nonstereotyped.2))) 

vuong(m.stereotyped.2, m.nonstereotyped.2)
</code></pre>

<pre><code>## Vuong Non-Nested Hypothesis Test-Statistic: 1.545984 
## (test-statistic is asymptotically distributed N(0,1) under the
##  null that the models are indistinguishible)
## in this case:
## model1 &gt; model2, with p-value 0.061054
</code></pre>

<p>The Vuong test also suggested that the stereotyped model had better fit. This
stereotyping effect can be seen as the overall s-shape of the data compared to a
line with slope = 1 (we would expect a line with slope = 1 if the data did not
exhibit stereotype exaggeration).</p>

<h4>Systematic over-/under- representation across careers</h4>

<p>We can estimate overrepresentation of a gender across careers from our logistic
regression model by testing to see if the coefficient of the intercept is
significantly different from 0 when the x-intercept is set to 50% women in the
BLS. Since we fit the model with that x-intercept already, it suffices to
examine the intercept of the fitted model:</p>

<pre><code class="r">#regression model coefficients and confidence intervals
summary(m.stereotyped)
</code></pre>

<pre><code>## 
## Call:
## glm(formula = search_p_women ~ I(bls_p_women - 0.5), family = quasibinomial, 
##     data = proportions, weights = search_n)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -6.5619  -1.4354   0.1609   1.5076   3.9135  
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)           -0.2604     0.0971  -2.682   0.0103 *  
## I(bls_p_women - 0.5)   5.3307     0.3773  14.130   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasibinomial family taken to be 4.467219)
## 
##     Null deviance: 1594.45  on 44  degrees of freedom
## Residual deviance:  204.08  on 43  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 4
</code></pre>

<pre><code class="r">confint(m.stereotyped)
</code></pre>

<pre><code>## Waiting for profiling to be done...
</code></pre>

<pre><code>##                           2.5 %      97.5 %
## (Intercept)          -0.4511975 -0.07004516
## I(bls_p_women - 0.5)  4.6151838  6.09629887
</code></pre>

<pre><code class="r">#coefficients as odds ratios
exp(coef(m.stereotyped))
</code></pre>

<pre><code>##          (Intercept) I(bls_p_women - 0.5) 
##            0.7707274          206.5835542
</code></pre>

<pre><code class="r">exp(confint(m.stereotyped))
</code></pre>

<pre><code>## Waiting for profiling to be done...
</code></pre>

<pre><code>##                           2.5 %      97.5 %
## (Intercept)            0.636865   0.9323517
## I(bls_p_women - 0.5) 101.006392 444.2106410
</code></pre>

<pre><code class="r">#coefficients as percentages at 50% women
inv.logit(coef(m.stereotyped))
</code></pre>

<pre><code>##          (Intercept) I(bls_p_women - 0.5) 
##            0.4352603            0.9951827
</code></pre>

<pre><code class="r">inv.logit(confint(m.stereotyped))
</code></pre>

<pre><code>## Waiting for profiling to be done...
</code></pre>

<pre><code>##                          2.5 %    97.5 %
## (Intercept)          0.3890761 0.4824959
## I(bls_p_women - 0.5) 0.9901967 0.9977539
</code></pre>

<p>Indeed, we find that the intercept does have a significant effect in this
model (estimated effect: -0.26, 95% confidence interval: [-0.45, -0.07], t43 =
-2.68, p &lt; 0.05);<sup><a name="footnote3_source" href="#footnote3">3</a></sup>
this effect can be seen in Figure 2 as a dip in the predicted proportion of
women in the search results at (50%, 50%). This effect corresponds to an odds
ratio of approximately 0.77 (95% CI: [0.64, 0.93]); this means that in a
profession with 50% women, we would expect about 45% of the images to be women
on average (95% CI: [38.9%, 48.2%]).</p>

<p>The particular combination of stereotype exaggeration and underrepresentation
of women that we see – slight pulling at the extremes and slight bias towards
male images – combine to affect male- and female-dominated professions
differently. In male-dominated professions (lower-left quadrant of stereotyped
model plot) both effects amplify each other, so a higher proportion of males
appear in search results than are in the BLS. By contrast, in female-dominated
professions (upper-right quadrant) these two effects essentially cancel each
other out, leading to a similar proportion of women in the search results as are
in the BLS.</p>

<h2>Study 2: Differences in qualitative representation</h2>

<p>Search results can be biased even when their gender proportions are
representative. For example, in reviewing the images collected for Study 1, we
identified many examples of sexualized depictions of women who were almost
certainly not engaged in the profession they portrayed; we dub this the <strong>sexy
construction worker problem</strong>, as images of female construction workers in our
results tended to be sexualized caricatures of construction workers. We wished
to assess whether images that better match the stereotypical gender of a
profession were systematically portrayed as more or less professional,
attractive, or appropriate. Note that while there are many interesting
differences to unpack here, our primary focus is on assessing these
characteristics so that we can control for them in subsequent analysis.</p>

<h3>Methods</h3>

<p>We used the top 8 male and female images from each profession, as these images
will be used again in study 3, below. Initially, we piloted a study in which
people were asked to give 5 adjectives describing the person in each image, but
found that this task was too difficult. We therefore opted to select 8
adjectives derived from our pilot results and our research questions:
attractive, provocative, sexy, professional, competent, inappropriate,
trustworthy, and weird. We then had turkers indicate on a 5-point scale
(strongly disagree to strongly agree) whether they felt each adjective described
the person in the picture. Each turker could rate each image at most once,
though no turker could rate more than 600 images. Each image was rated by at
least 3 turkers.</p>

<h3>Results</h3>

<p>At a high level, we found that images showing a person matching the majority
gender for a profession tend to be ranked as slightly more professional-looking
and slightly less inappropriate than those going against stereotype.</p>

<h3>Adjective ratings</h3>

<p>One would expect that men and women rate images differently; however, this is
not our focus here, so we have attempted to factor out these differences. We
conducted a series of mixed-effects ordinal logistic regressions to model how
turkers rated images for each adjective. We included the turker’s gender, the
image gender, and their interaction as fixed effects; we included the turker and
the image as random effects.<sup><a name="footnote4_source"
href="#footnote4">4</a></sup> This allows our models to account for (for
example) situations where women systematically rate men as more attractive than
men do. </p>

<pre><code class="r">adjective_ratings = read.csv(&quot;data/public/image_adjective_ratings.csv&quot;)

#make adjectives ordered factors
adjectives = colnames(adjective_ratings)[9:16]
adjective_ratings[,adjectives] = llply(adjective_ratings[,adjectives], function (adjective) 
    ordered(adjective, levels=c(&quot;strongly disagree&quot;, &quot;disagree&quot;, &quot;neutral&quot;, &quot;agree&quot;, &quot;strongly agree&quot;)))

#build a list mapping each adjective name onto its ordinal model
m.adjectives = llply(adjectives, function(adjective)
    clmm(adjective_ratings[[adjective]] ~ turker_gender * image_gender + (1|turker) + (1|image_url), 
        data=adjective_ratings))
names(m.adjectives) = adjectives
</code></pre>

<p>We used the coefficients of the image effect in each model as a
normalized rating for that adjective. These ratings have the effects of turker,
turker gender, image gender, and their interaction factored out and are all
approximately standard normally distributed.</p>

<h3>Stereotyping bias in qualitative ratings</h3>

<p>We hypothesized that images matching the typical gender of a given profession
might be portrayed differently from images that do not match the typical gender
of that profession (as in the sexy construction worker problem). To assess the
whether this was the case, we ran linear mixed-effects regressions, each with
one of the adjective ratings derived above as the independent variable. Each
model included image gender, the image gender proportion in BLS (the % of people
in the BLS matching the gender of the image; e.g. for a male image of a
construction worker, this would be the % of construction workers who are male
according to the BLS), and the interaction of these two terms as fixed effects.
The models also included the occupation as a random effect. As noted above, we
are primarily interested in these factors as controls in Study 3 (below), so we
only summarize two high-level trends in the results here.</p>

<p>First, adjectives like professional (F1,623.6 = 36.6, p &lt; 0.0001), competent
(F1,630 = 28.4, p &lt; 0.0001), and trustworthy (F1,627.8 = 33.8, p &lt; 0.0001) had
significantly higher ratings when the proportion of people in the BLS matching
the gender of the image was higher. Second, adjectives like inappropriate
(F1,635.2 = 20.4, p &lt; 0.0001) or provocative (F1,635.12 = 4.38, p &lt; 0.05) had
significantly lower ratings when the proportion of people in the BLS matching
the gender of the image was higher. In other words, we again we see an effect of
stereotyping exaggeration: images matching the gender stereotype of a profession
tend to be slightly more professional-looking and slightly less inappropriate
than those going against stereotype. The reason for this effect is unclear: it
may be that these images are rated less professional/appropriate because of
raters’ biases against images going against their stereotypes for those
professions. However, it may also be that these depictions are of lower quality
– examples of the sexy construction worker problem, where depictions against
stereotype are not true depictions of the profession at all.</p>

<h2>Study 3: Perceptions of search results</h2>

<p>Having described the bias present in image search results for careers –
stereotype exaggeration and differences in representation – we next turn our
attention to whether these differences affect people’s appraisals of the quality
of search results and, in a hypothetical task, what image they choose to
represent an occupation. This is not a purely abstract problem: a textbook
publisher recently recalled a math textbook after discovering they had selected
an image from a pornographic film for the cover [19].</p>

<p>We generated synthetic sets of image search results for each occupation, in
which the gender balance was manipulated by re-ranking images from the original
search results. Each synthetic result had a different gender distribution, with
8 images in each result. For each search term we generated up to 7 synthetic
results: all men, all women, equal proportions, proportions from Google search,
proportions from the BLS, the reverse of the proportions from Google search, and
the reverse of the proportions in the BLS.</p>

<p>To ensure that the proportion of women in the BLS for a given search term does
not influence the proportion of images in the synthetic results for that search
term, synthetic subsets (other than equal) were only included if their
corresponding reversed subset could also be included (for example, if we had
enough women to make a synthetic search result with 6/8 images of women, but not
enough men to make a synthetic search result with 6/8 images of men, neither
synthetic result was included). This ensures that if gender has no effect on the
probability of an image being selected, the baseline probability of two images
of different gender being selected for any occupation will be the same: <U+215B>
(regardless of the gender ratio in that occupation).</p>

<p>To generate a subset with k women, we selected the top k female images from our
labelled dataset (in the order they appeared in the original Google image search
results) and the top 8-k male images. The images were displayed to participants
in the order they appeared in the original search results. Participants could
view one result set per occupation. This was to prevent participants from
realizing that we manipulated the gender proportions of the search results, as
they might if they saw multiple synthetic results for the same occupation with
very different gender distributions.</p>

<p>On viewing a result set, we asked participants to select one image to
illustrate the occupation for a hypothetical business presentation. We then
asked them to describe how well the image results matched the search term (the
occupation), in a drop down from 1 (very poor) to 5 (very good), and to describe
why they rated as they did.</p>

<h3>Image Selection Results</h3>

<p>We used logistic regression to model the probability that a given image is
selected as the best result by a participant. Our model included image gender,
the image gender proportion in BLS (see explanation under Study 2), participant
gender, and their interactions. We also included all of the image adjective
ratings to control for differences in qualitative representation. Results are
shown in Table 1.</p>

<h4>Over-/under- representation and participant effects</h4>

<p>We found no evidence of systematic over-/under- representation of either gender
(there were no significant effects of image gender). Neither were there
significant effects of participant gender (suggesting men and women generally
judge the best search result in the same way), nor any significant interactions
with either of these factors.</p>

<h4>Stereotype exaggeration</h4>

<p>As with the gender distributions in the search results themselves, we found
evidence of stereotyping when people choose image results: image gender
proportion in BLS had a significant effect on the probability of an image being
selected; i.e., an image matching the majority gender proportion of its
occupation was more likely to be selected. We believe this is consequence of
stereotype matching: an image matching a person’s stereotype for that gender is
more likely to be selected as an exemplar result.</p>

<p><code>Table 1. Factors affecting image selection in Study 3. Coefficients are on a
logit scale. Note the stereotype effect: greater % image gender in BLS is
associated with higher probability that an image is selected.</code></p>

<p><code>Table 2. Factors affecting search result quality ratings in Study 3.
Coefficients are on a logit scale.</code></p>

<h3>Search Result Quality Rating Results</h3>

<p>We saw very similar effects influencing quality rating. We ran a mixed effects
ordinal logistic regression to model quality rating based on proportion of women
in BLS, proportion of women in the synthetic search result, participant gender,
and their interactions. We included the mean of the image adjective ratings in
the synthetic search result to control for differences in qualitative
representation. We also included participant and search term (occupation) as
random effects. Results are in Table 2.</p>

<h4>Over-/under- representation and participant effects</h4>

<p>As above, we found no significant over-/under-representation effect: in an
occupation with 50% women, we would not expect an all-male search result to be
rated differently from an all-female search result (estimated difference =
-0.41, SE = 0.47, z = -0.87, p = 0.38). As above, there were no significant
effects of participant gender.</p>

<h4>Stereotype exaggeration</h4>

<p>We again saw a stereotype exaggeration effect, manifested here as a significant
interaction between proportion of women in BLS and proportion of women in the
search result: in male-dominated occupations, search results with more males are
preferred; in female-dominated occupations, search results with more females are
preferred.</p>

<p>Viewed from the perspective of this task, these results make sense: we asked
people to select the best search result (or to rate the quality of all results),
and they tended to prefer images matching their mental image of each profession,
both in qualitative characteristics and in expected gender. This reflects the
strong sense that people have of expected gender proportions in a broad spectrum
of occupations, which we explore in more detail next. This also emphasizes an
important tension between possible broader societal goals in manipulating gender
as a design dimension in search results versus end-users’ quality expectations,
an issue we discuss in detail at the end of this paper.</p>

<h2>Study 4: Perceptions of gender proportions in occupations</h2>

<p>Finally, we sought to understand whether and how gender proportions in image
search results can affect people’s perceptions of the actual prevalence of men
and women in different occupations, both to understand how existing
(stereotype-exaggerating, misrepresented) results might be affecting people’s
perceptions of gender proportions and how feasible manipulating gender
distributions in image search might be as a method for affecting those
perceptions. This gets at a primary motivation of our paper: opening up gender
proportions as a design dimension in image search.</p>

<p>Given the many possible day-to-day influences on perceptions of the prevalence
of genders in different fields, we chose to collect people’s baseline
perceptions, wait two weeks, show them a synthetic image search result set for
the same career, and then immediately ask them their perceptions of prevalence.</p>

<p>We asked each participant the demographics information we used in studies 2 and
3. Then for each career we asked what percent of people working in that career
in the US are women, alongside three distraction questions: what education they
believe is typical for someone in that career, whether they believe the career
was growing, and how prestigious they think it is. Participants could answer for
as many careers as they wished.</p>

<p>After two weeks, each participant received an email thanking them for their
prior participation and inviting them to participate in a new round of tasks; we
limited access both in the script that managed our tasks and using an assigned
qualification on Mechanical Turk. For each profession to which they had
previously responded, we returning participants to view a synthetic search
result and complete the image search task from study 3; on the next page we
re-asked the four questions from the first page: typical education for the
career, percent women, whether the career was growing, and its prestige.</p>

<h3>Results</h3>

<h4>Perceptions absent influence</h4>

<p>People’s initial perceptions of gender proportions in occupations are quite
good. We assessed the correlation of their existing perceptions to real-world
proportions using a mixed-effects linear regression with gender proportions in
BLS as the fixed effect and participant as a random effect. The marginal
pseudo-R<sup>2</sup> of this model was 0.717 (F1,297.71 = 870.21, p &lt; 0.0001).</p>

<h4>Perceptions after influence</h4>

<p><code>Table 3. Effects of the manipulated search result and a person’s pre-existing
opinion of % women in an occupation on their opinion after seeing the
manipulated result (Study 4).</code></p>

<p>After exposure to search results with manipulated gender proportions, estimates
shifted slightly in the direction of the manipulated proportions. We ran a
linear mixed-effects regression to predict perceived gender proportions, with
initial perceived gender proportion and manipulated search gender proportion as
fixed effects and participant as a random effect. Both fixed effects were
significant: while a person’s original perceptions of an occupation dominated
their opinion two weeks later; approximately 7% of a person’s subsequent opinion
on average was determined by the result set they were exposed to (p &lt; 0.01, see
Table 3).</p>

<p>While this only shows short-term movement due to manipulated search results,
cultivation theory suggests that long-term, ongoing exposure to such results
might shift perceptions over time. This suggests that there may be value in
considering gender distribution as a more deliberate design dimension in image
search results, as we discuss next.</p>

<h2>Discussion</h2>

<p>Our results provide guidance on the short-term effects of possible changes to
search engine algorithms and highlight tensions in possible designs of search
algorithms.</p>

<h3>As a design space, what other kinds of search results could we design and what might be the consequences?</h3>

<p>There are two sets of adjustments that can be made: adjusting the gender
distribution, and adjusting the distribution of qualitative image
characteristics within genders (e.g. increasing the proportion of female
construction worker results that are rated as professional or competent to
correct for the sexy construction worker problem). Taking the former as a
starting point, we outline three possible (amongst many) ways of adjusting
search results: 1) exaggerating, or continuing to accept, exaggerated gender
stereotypes; 2) aligning the gender balance of results to match reality; or 3)
balancing the genders represented in results.</p>

<p>These models also surface several design tensions in this space. In particular,
we might ask if our goal is to improve perceptions of search result quality, or
to advance a broader social agenda to shift perceptions of gender equality in
various professions. While potentially at odds in the short term (e.g., highly
stereotyped results might be highly rated but not have desirable societal
effects), cultivation theory also suggests these goals may not be as contrary
over the long term if perceptions can be shifted to match a broader goal of
equal representation (as, at least in the short term, Study 4 suggests is
possible). We discuss how these motivations interact in more detail for each
proposed model. We do not wish to come down on any side of these issues, but
wish to advance an understanding of how the choices people make in designing
algorithms can (and already do) define a design space that explicitly or
implicitly affects these issues.</p>

<ol>
<li><p><strong>Stereotype model</strong>. Exaggerate gender stereotypes. This might improve
subjective assessment of quality over baseline results if the dominant gender is
already represented as professional and appropriate, so would likely not require
correcting for qualitative differences (simplifying the technical problem). This
would also give more exemplars for the selection task. At the same time,
continuing to portray careers as more gender segregated than they are, or even
further exaggerating gender imbalances, has the disadvantage of potentially
influencing people’s perceptions of occupation gender distributions over time to
be less accurate and reinforcing stereotypes that can shape and limit career
aspirations and treatment.</p></li>
<li><p><strong>Reality model</strong>. Correct the slight stereotype exaggeration and
underrepresentation of women seen in the data so that gender distributions
better resemble the BLS. So long as we can select images of the non-dominant
gender that have high professionalism and low inappropriateness, this would at
least better represent the reality of the profession while having little effect
on the perceived search result quality or the selection task. Over the long
term, exposure to such results might improve people’s estimations of real-world
gender proportions in occupations. This also represents only a small
perturbation to existing results, and may not even require adjustments to
distributions to account for qualitative differences in representation due to
how close the existing search proportions are to actual BLS proportions.</p></li>
<li><p><strong>Balanced model</strong>. Adjust the gender proportions in occupations to be equal or
closer to equal. This may impair the selection task by giving fewer gender
exemplars. However, if this is paired with corrections for qualitative
differences in representation so that portrayals of the non-dominant gender are
similar to the dominant one (particularly for professionalism), we do not
believe it would significantly degrade people’s overall perceptions of the
quality of the results. This model exposes a tension between a desire for
results perceived as high-quality and possible societal goals for advancing
equal representation. While the short-term effects on perceived search result
quality would likely be negative, both cultivation theory [32] and the results of
study 4 predict that this could, in the long term, shift people’s perceptions
towards a less gender-stereotyped view of these professions. Along with that
long-term shift, a possible result may be that perceptions of quality shift back
as people begin to perceive gender proportions as more equal.</p></li>
</ol>

<h3>Feasibility of Manipulating Representation</h3>

<p>Automatic gender classification of images of people is an outstanding problem.
While state-of-the art classifiers perform well under certain circumstances,
they have historically focused on straight-on images of a single face, typically
without visual occlusions, uneven lighting, or complicated backgrounds [26].
However, recent studies on gender classification on images collected in the wild [1,31]
or of only partial features [24] strongly suggest that automated solutions will
soon reach or surpass the accuracy of human annotators. Meanwhile, automated
human labelling of these images would not be much more costly than the data
collection processes used in this paper, and would provide ground truth data for
future automated approaches.</p>

<h3>Limitations and Future Work</h3>

<p>We have focused here on search results absent personalization or other
additional context (such as time of year or location) that may affect results
shown to users in real-world tasks. While the businessperson making a slide
presentation might be seeking an accurate depiction of a given profession, other
users with different goals might prefer caricatured or inaccurate portrayals,
such as the sexy construction worker. We also do not address the cause of the
biases and misrepresentation found (e.g., are these due to actual prevalence in
webpages, or due to the ranking algorithms used by Google?). Future work might
try to tease these effects apart, and to investigate these phenomena in other
types of image search, such as on photo sharing sites and in social media. To
aid in replication, our data and code are available online 
<a href="https://github.com/mjskay/gender-in-image-search">here</a>.</p>

<h2>Conclusion</h2>

<p>Academics and the technology community have raised concerns about potential
biases in search engines and in stock photos. We contribute an assessment of
gender representation in image search results and its effects on perceived
search result quality, images selected, and perceptions about reality. We find
that image search results for occupations slightly exaggerate gender stereotypes
and portray the minority gender for an occupational less professionally. There
is also a slight underrepresentation of women. This stereotype exaggeration is
consistent with perceptions of result quality – people believe results are
better when they agree with the stereotype – but risks reinforcing or even
increasing perceptions of actual gender segregation in careers.</p>

<p>Addressing concerns such as these in search engines and other information
sources, however, requires balancing design tensions. For example, maintaining
perceived search quality and accurately representing available materials may be
at odds with supporting socially desirable outcomes and representing either
real-world distributions of careers or idealized distributions of careers. We
hope to advance a constructive discussion on gender representation as a design
dimension (explicit and implicit) in information systems.</p>

<h2>Acknowledgements</h2>

<p>We thank Gilbert Bernstein and Benjamin Mako Hill for their valuable feedback
on this work.</p>

<h2>Footnotes</h2>

<ol>
<li><p><a name="footnote1" href="#footnote1_source">^</a> We use “turkers” for
studies 1 and 2, where they were asked only to label data, and “participants”
for studies 3 and 4, where their opinions and perceptions were
solicited.</span></p></li>
<li><p><a name="footnote2" href="#footnote2_source">^</a> As the BLS uses only
binary gender classifications, we also restricted labels to binary gender
classification here.</p></li>
<li><p><a name="footnote3" href="#footnote3_source">^</a> This test was carried out
using the stereotyped model, but we note that a similar test carried out on
the non-stereotyped model yielded similar results and confidence intervals.</p></li>
<li><p><a name="footnote4" href="#footnote4_source">^</a> While we have used worker
and image as random effects here and elsewhere in the paper, where estimable
we have also compared results with fixed effects models and found similar
effects. We believe random effects to be more appropriate here as some
workers have completed only a small number of tasks.</p></li>
</ol>

<h2>References</h2>

<ol>
<li><p>Arigbabu OA; Ahmad SMS, Adnan WAN, Yussof S, Iranmanesh V, Malallah, FL,
Gender recognition on real world faces based on shape representation and
neural network. ICCOINS 2014.</p></li>
<li><p>Baker P, Potts A. “Why do white people have thin lips?” Google and the
perpetuation of stereotypes via auto-complete search forms. Crit Disc St
10(2): 187-204.</p></li>
<li><p>Behm-Morawitz E, Mastro D. The Effects of the Sexualization of Female Video
Game Characters on Gender Stereotyping and Female Self-Concept. Sex Roles
2009; 61(11-12): 808-823.</p></li>
<li><p>Bodenhausen GV, Wyer RS. Effects of stereotypes in decision making and
information-processing strategies. J Pers Soc Psychol 1985; 48(2): 267.</p></li>
<li><p>Bureau of Labor Statistics. Labor Force Statistics from the Current
Population Survey, Section 11. 5 February 2013.
<a href="http://www.bls.gov/cps/aa2012/cpsaat11.htm">http://www.bls.gov/cps/aa2012/cpsaat11.htm</a>.</p></li>
<li><p>Coltrane S, Adams M. Work–family imagery and gender stereotypes: Television
and the reproduction of difference. J Vocat Behav 1997; 50(2): 323-347.</p></li>
<li><p>Correll SJ. Gender and the career choice process: The role of biased
self-assessments. Am J Sociol 2001; 106(6): 1691–1730.</p></li>
<li><p>Correll SJ. Constraints into preferences: Gender, status, and emerging
career aspirations. Am Sociol Rev 2004; 69(1): 93-113.</p></li>
<li><p>Executive Office of the President. Big Data: Seizing Opportunities,
Preserving Values. May 2014.</p></li>
<li><p>Friedman B, Nissenbaum H. Bias in computer systems. ACM T Inform Syst 1996;
14(3): 330-347.</p></li>
<li><p>Gerbner G, Gross L, Morgan M, Signorielli N. Living with television: The
dynamics of the cultivation process. Perspectives on media effects 1986: 17-40.</p></li>
<li><p>Graves SB. Television and Prejudice Reduction: When Does Television as a
Vicarious Experience Make a Difference? J Soc Issues 1999; 55(4): 707-727.</p></li>
<li><p>Grossman P. New Partnership with LeanIn.org. InFocus by Getty Images.
<a href="http://infocus.gettyimages.com/post/new-partnership-with-leaninorg">http://infocus.gettyimages.com/post/new-partnership-with-leaninorg</a>.</p></li>
<li><p>Halpert JA, Wilson ML, Hickman JL. Pregnancy as a source of bias in
performance appraisals. J Organ Behav 1993.</p></li>
<li><p>Haslam SA, Turner JC, Oakes PJ, Reynolds KJ, Doosje, B From personal
pictures in the head to collective tools in the word: how shared stereotypes
allow groups to represent and change social reality. In C McGarty, VY Yzerbyt, R
Spears (eds.). Stereotypes as explanations: The formation of meaningful beliefs
about social groups 2002. Cambridge University Press, 157-185.</p></li>
<li><p>Heilman ME. Description and prescription: How gender stereotypes prevent
women&#39;s ascent up the organizational ladder. J Soc Issues 2001; 57(4): 657-674.</p></li>
<li><p>Heilman ME. Okimoto, TG. 2008. Motherhood: A potential source of bias in
employment decisions. J Appl Psychol 2008; 93(1): 189-198.</p></li>
<li><p>Hilton JL, &amp; Von Hippel W. Stereotypes. Annu Rev Psychol 1996; 47(1):
237-271.</p></li>
<li><p>Hooper B. Porn star appears on cover of Thai math textbook. United Press
International. <a href="http://upi.com/5031410787947">http://upi.com/5031410787947</a></p></li>
<li><p>Introna L, Nissenbaum H. Defining the web: The politics of search engines.
Computer 2000; 33(1): 54-62.</p></li>
<li><p>Jacobs J. Gender Inequality at Work. Thousand Oaks, CA: SAGE Publications,
1995.</p></li>
<li><p>Kammerer,Y, Gerjets P. How search engine users evaluate and select Web
search results: The impact of the search engine interface on credibility
assessments. Libr Inform Sci 2012; 4: 251–279.</p></li>
<li><p>Keane MT, O&#39;Brien M, Smyth B. Are people biased in their use of search
engines? Commun ACM 2008; 51(2): 49-52.</p></li>
<li><p>Khorsandi R, Abdel-Mottaleb M. Gender classification using 2-D ear images
and sparse representation. 2013 IEEE Workshop on Applications of Computer Vision
(WACV), 461-466.</p></li>
<li><p>Lean In Foundation. Getty Image Collection. <a href="http://leanin.org/getty">http://leanin.org/getty</a>.</p></li>
<li><p>Makinen E, Raisamo R. Evaluation of Gender Classification Methods with
Automatically Detected and Aligned Faces, IEEE T Pattern Anal 2008; 30(3):
541-547.</p></li>
<li><p>Massey D. Categorically Unequal: The American Stratification System. NY:
Russell Sage Foundation, 2007.</p></li>
<li><p>Miller CC. 10 February 2014. LeanIn.org and Getty Aim to Change Women’s
Portrayal in Stock Photos. New York Times, B3. <a href="http://nyti.ms/1eLY7ij">http://nyti.ms/1eLY7ij</a></p></li>
<li><p>Pariser E. The Filter Bubble: What the Internet Is Hiding from You. 2011.
Penguin Press.</p></li>
<li><p>Potter WJ. Cultivation theory and research, Hum Commun Res 1993; 19(4):
564-601.</p></li>
<li><p>Shan C. Learning local binary patterns for gender classification on
real-world face images, Pattern Recogn Lett 2012; 33(4), 431-437.</p></li>
<li><p>Shrum LJ. Assessing the Social Influence of Television A Social Cognition
Perspective on Cultivation Effects. Commun Res 1995; 22(4): 402-429.</p></li>
<li><p>Spencer SJ, Steele CM, Quinn DM. Stereotype threat and women&#39;s math
performance. J Exp Soc Psychol 1999; 35(1): 4-28.</p></li>
<li><p>Snyder M, Tanke ED, Berscheid E. Social perception and interpersonal
behavior: On the self-fulfilling nature of social stereotypes. J Pers Soc
Psychol 1977; 35(9): 656–666.</p></li>
<li><p>Sweeney L. Discrimination in online ad delivery. Commun ACM 2013; 56(5):
44-54.</p></li>
<li><p>Tajfel H. Social stereotypes and social groups. In Turner JC, Giles H.
Intergroup Behaviour 1981. Oxford: Blackwell. 144–167.</p></li>
<li><p>Vaughan L, Thelwall M. Search engine coverage bias: evidence and possible
causes. Inform Process Manag 2004; 40(4): 693-707.</p></li>
<li><p>Williams D. Virtual Cultivation: Online Worlds, Offline Perceptions. J
Commun 1996; 56(1): 69–87.</p></li>
<li><p>Word CO, Zanna MP, Cooper J. The nonverbal mediation of self-fulfilling
prophecies in interracial interaction. J Exp Soc Psychol 1974; 10(2): 109–120.</p></li>
<li><p>X Tang, K Liu, J Cui, F Wen, X Wang. IntentSearch: Capturing User Intention
for One-Click Internet Image Search, IEEE T Pattern Anal 34(7): 1342-1353.</p></li>
<li><p>Zha ZJ, Yang L, Mei T, Wang M, Wang Z, Chua TS, Hua XS. 2010. Visual query
suggestion: Towards capturing user intent in internet image search. ACM T Multim
Comput 2010; 6(3): a13.</p></li>
</ol>

</body>

</html>
